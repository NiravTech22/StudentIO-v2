version: '3.8'

services:
  # 1b. Julia Reasoning Core
  julia:
    build:
      context: core/julia_service
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
    restart: on-failure

  # 1. Core AI Service (Python)
  core:
    build:
      context: .
      dockerfile: core/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PORT=8000
      - CORSM_ORIGINS=*
      - MODEL_DEVICE=cpu
      - JULIA_URL=http://julia:8080
    volumes:
      - hf_cache:/root/.cache/huggingface # Persist AI models (Download once!)
    restart: on-failure

  # 2. Backend API (Node.js)
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    ports:
      - "3000:3000"
    environment:
      - PORT=3000
      - AI_SERVICE_URL=http://core:8000
    depends_on:
      - core
    restart: on-failure

  # 3. Frontend (Nginx/Vite)
  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    ports:
      - "80:80"
    depends_on:
      - backend

volumes:
  hf_cache: # Named volume for model persistence
